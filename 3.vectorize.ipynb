{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.3"
    },
    "colab": {
      "name": "3.vectorize.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Ap59V9Ae2G5"
      },
      "source": [
        "# Utilisation de vectorize et guvectorize\n",
        "\n",
        "<img src=\"https://github.com/lsteffenel/cours_numba_2017/blob/master/figures/numba_blue_icon_rgb.png?raw=1\" alt=\"Drawing\" style=\"width: 20%;\"/>\n",
        "\n",
        "<center>**Loic Gouarin**</center>\n",
        "<center>*8 novembre 2017*</center>\n",
        "\n",
        "Numba fournie deux autres fonctionalités (**vectorize** et **guvectorize**) permettant d'accélérer des fonctions universelles NumPy ([ufuncs](https://docs.scipy.org/doc/numpy/reference/ufuncs.html)). Avant d'aller plus loin, rappelons ce qu'est une ufunc.\n",
        "\n",
        "Une ufunc est une fonction qui agit élément par élément pour un tableau donné. Elle permet de vectoriser les opérations et elle est définie de la manière suivante: les paramètres d'entrée sont des scalaires et le paramètre de sortie est également un scalaire. \n",
        "\n",
        "Prenons par exemple la fonction heaviside qui est définie de la manière suivante\n",
        "\n",
        "$$\n",
        "heaviside(x, h_0) = \n",
        "\\left\\{\n",
        "\\begin{array}{l}\n",
        "0 \\; \\text{si} \\; x<0, \\\\\n",
        "h_0 \\; \\text{si} \\; x=0, \\\\\n",
        "1 \\; \\text{si} \\; x>0.\n",
        "\\end{array}\n",
        "\\right.\n",
        "$$\n",
        "\n",
        "Les ufuncs sont codées en C dans NumPy et sont donc optimisées. Voici la version de la fonction heaviside dans NumPy (attention, cette fonction n'est disponible qu'à partir de la dernière version: 1.13)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zHwkhkPZe2G_"
      },
      "source": [
        "```C\n",
        "/**begin repeat\n",
        " * #type = npy_float, npy_double, npy_longdouble#\n",
        " * #c = f, ,l#\n",
        " * #C = F, ,L#\n",
        " */\n",
        "\n",
        "@type@ npy_heaviside@c@(@type@ x, @type@ h0)\n",
        "{\n",
        "    if (npy_isnan(x)) {\n",
        "        return (@type@) NPY_NAN;\n",
        "    }\n",
        "    else if (x == 0) {\n",
        "        return h0;\n",
        "    }\n",
        "    else if (x < 0) {\n",
        "        return (@type@) 0.0;\n",
        "    }\n",
        "    else {\n",
        "        return (@type@) 1.0;\n",
        "    }\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WlS0M0yre2HA"
      },
      "source": [
        "Comme vous pouvez le voir ce n'est pas très lisible et surtout, vous n'avez certainement pas envie de faire une interface C pour une fonction simple.\n",
        "\n",
        "**vectorize** et **guvectorize** sont donc là pour vous aider à construire des ufuncs tout en restant en Python.\n",
        "\n",
        "## vectorize\n",
        "\n",
        "Voici comment la fonction heaviside s'écrit en Numba en utilisant **vectorize**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "iCV30tn-e2HA"
      },
      "source": [
        "from numba import vectorize\n",
        "\n",
        "@vectorize(['float64(float64, float64)'])\n",
        "def heaviside(x, h0):\n",
        "    if x == 0:\n",
        "        return h0\n",
        "    elif x < 0:\n",
        "        return 0.\n",
        "    else:\n",
        "        return 1."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "reieKDjee2HC"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "x = -1 + 2*np.random.random(1000000)\n",
        "h0 = .4 \n",
        "numba_res = heaviside(x, h0)\n",
        "numpy_res = np.heaviside(x, h0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mPWFRa7ye2HD"
      },
      "source": [
        "np.all(numba_res == numpy_res)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fJ6diBH1e2HE"
      },
      "source": [
        "%timeit heaviside(x, h0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P-dl0UJ6e2HF"
      },
      "source": [
        "%timeit np.heaviside(x, h0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ENQ0AeB4e2HG"
      },
      "source": [
        "La fonction Numba est plus lente que la fonction NumPy. On peut accélérer un peu les choses en spécifiant la cible. En effet, **vectorize** a un mot clé **target** indiquant\n",
        "\n",
        "- 'cpu'\n",
        "\n",
        "fonction pour un thread sur CPU\n",
        "\n",
        "- 'parallel'\n",
        "\n",
        "fonction multi-threads pour CPU\n",
        "\n",
        "- 'cuda'\n",
        "\n",
        "fonction utilisant cuda sur GPU\n",
        "\n",
        "Voyons ce que ça donne si nous mettons cette fonction en parallèle."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UXggsTHKe2HH"
      },
      "source": [
        "@vectorize(['float64(float64, float64)'], target='parallel')\n",
        "def heaviside_para(x, h0):\n",
        "    if x == 0:\n",
        "        return h0\n",
        "    elif x < 0:\n",
        "        return 0.\n",
        "    else:\n",
        "        return 1."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ytZaMk76e2HI"
      },
      "source": [
        "On l'appelle une première fois pour ne pas prendre en compte le temps de compilation dans le calcul de la performance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-heCmRdBe2HI"
      },
      "source": [
        "numba_res_para = heaviside_para(x, h0)\n",
        "np.all(numba_res_para == numpy_res)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kQvsy9Nqe2HJ"
      },
      "source": [
        "%timeit heaviside_para(x, h0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9xS2k-zie2HJ"
      },
      "source": [
        "Nous sommes cette fois un peu plus rapide que la fonction NumPy. Mais il faut noté que la version NumPy n'est pas parallèle."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "GaAT8r5ke2HJ"
      },
      "source": [
        "@vectorize(['float64(float64, float64)'], target='cuda')\n",
        "def heaviside_gpu(x, h0):\n",
        "    if x == 0:\n",
        "        return h0\n",
        "    elif x < 0:\n",
        "        return 0.\n",
        "    else:\n",
        "        return 1."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8iAwXouGe2HK"
      },
      "source": [
        "heaviside_gpu(x, h0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rcpXW5xde2HK"
      },
      "source": [
        "%timeit heaviside_gpu(x, h0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0pNn8Xbte2HL"
      },
      "source": [
        "Comme vu lors l'introduction à **@jit**, il est possible de spécialiser une fonction vectorize en donnant les différents types sous forme de liste."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "H1nl1Oeue2HL"
      },
      "source": [
        "@vectorize(['float64(float64, float64)',\n",
        "            'float32(float32, float32)',\n",
        "            'int32(int32, int32)'], target='parallel')\n",
        "def heaviside_para(x, h0):\n",
        "    if x == 0:\n",
        "        return h0\n",
        "    elif x < 0:\n",
        "        return 0.\n",
        "    else:\n",
        "        return 1."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Gz2b2AIe2HL"
      },
      "source": [
        "## guvectorize\n",
        "\n",
        "**guvectorize** fait exactement la même chose que **vectorize** à la seule différence que nous n'avons à aucun moment spécifié le tableau de sortie. Celui-ci est donc créé à chaque appel de la fonction. Ce qui peut avoir un coup non négligeable si nous appelons plein de fois la fonction. **guvectorize** permet de mettre le tableau de sortie dans les arguments.\n",
        "\n",
        "Reprenons notre exemple"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "ihv-vNHXe2HL"
      },
      "source": [
        "from numba import guvectorize    \n",
        "\n",
        "@guvectorize(['(float64[:], float64[:], float64[:])'], '(),()->()', target='parallel')\n",
        "def guheaviside(x, h0, y):\n",
        "    if x[0] == 0:\n",
        "        y[0] = h0[0]\n",
        "    elif x[0] < 0:\n",
        "        y[0] = 0.\n",
        "    else:\n",
        "        y[0] = 1.      "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yS0pwtoCe2HM"
      },
      "source": [
        "y = np.zeros_like(x)\n",
        "guheaviside(x, h0, y)\n",
        "np.all(y == numpy_res)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8swTV67je2HM"
      },
      "source": [
        "%timeit guheaviside(x, h0, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fXAVsYqZe2HN"
      },
      "source": [
        "**guvectorize** permet de gérer plus facilement le parallèlisme que **vectorize** lorsque vous ne faites pas des opérations élément par élément mais que vous avez des formules un peu plus complexes.\n",
        "\n",
        "Supposons que nous voulions calculer la racine carrée de la somme des éléments au carré des éléments colonnes d'une matrice. Ces calculs peuvent se faire de manière indépendante et de façon parallèle sur chacune des lignes.\n",
        "\n",
        "Voici comment l'écrire avec **guvectorize**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "_QJ-bmM1e2HN"
      },
      "source": [
        "from math import sqrt\n",
        "from numba import njit, guvectorize\n",
        "import numpy as np\n",
        "\n",
        "@njit\n",
        "def square_sum(arr):\n",
        "    a = 0.\n",
        "    for i in range(arr.size):\n",
        "        a = sqrt(a**2 + arr[i]**2)\n",
        "    return a\n",
        "\n",
        "@guvectorize([\"void(float64[:], float64[:])\"], \"(n) -> ()\", target=\"parallel\", nopython=True)\n",
        "def row_sum_gu(input_array, output_array) :\n",
        "    output_array[0] = square_sum(input_array)\n",
        "\n",
        "@njit\n",
        "def row_sum_jit(input_array, output_array) :\n",
        "    m, n = input_array.shape\n",
        "    for i in range(m) :\n",
        "        output_array[i] = square_sum(input_array[i,:])\n",
        "    return output_array\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kwU_AUSPe2HN"
      },
      "source": [
        "rows = int(64)\n",
        "columns = int(1e6)\n",
        "\n",
        "input_array = np.random.random((rows, columns))\n",
        "output_array = np.zeros((rows))\n",
        "output_array2 = np.zeros((rows))\n",
        "\n",
        "# Warmup an check that they are equal \n",
        "np.allclose(row_sum_jit(input_array, output_array), row_sum_gu(input_array, output_array2))\n",
        "%timeit row_sum_jit(input_array, output_array.copy())  # 10 loops, best of 3: 130 ms per loop\n",
        "%timeit row_sum_gu(input_array, output_array.copy())   # 10 loops, best of 3: 35.7 ms per loop"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8YWm-8mCe2HO"
      },
      "source": [
        "Le calcul du carré et de la racine carrèe ont un coût de calcul important. On voit ici tout le bénéfice d'utiliser **guvectorize** pour profiter du parallèlisme.\n",
        "\n",
        "## Exercice\n",
        "\n",
        "Proposez une version de cette fonction en utilisant **guvectorize**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "q8Gz-wx-e2HO"
      },
      "source": [
        "def splint(xa, ya, y2a, x, y):\n",
        "    n = xa.shape[0]\n",
        "    for i in range(x.shape[0]):\n",
        "        klo = 0\n",
        "        khi = n-1\n",
        "        while(khi-klo) > 1:\n",
        "            k = (khi+klo) >> 1\n",
        "            if xa[k] > x[i]:\n",
        "                khi = k\n",
        "            else:\n",
        "                klo = k\n",
        "        h = xa[khi] - xa[klo]\n",
        "        a = (xa[khi]-x[i])/h    \n",
        "        b = (x[i]-xa[klo])/h\n",
        "        y[i,:] = a*ya[klo,:]+b*ya[khi,:]+((a**3-a)*y2a[klo,:]+(b**3-b)*y2a[khi,:])*h**2/6."
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}